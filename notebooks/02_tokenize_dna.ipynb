{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d323c0c-226b-4bfc-9597-434bac7f303c",
   "metadata": {},
   "source": [
    "# Tokenize DNA using Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189d9926-2bc9-402d-94b1-4b02dbd1b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e1db56-3340-48a6-b5ef-8a5a97030e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "from adna.pylib import consts, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "489085ad-8ab3-47e3-b95a-216310c9de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQS, _ = datasets.read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d2a94-cded-44f8-8515-604dfa165e47",
   "metadata": {},
   "source": [
    "## What characters are used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31f420d-8738-4a5c-a376-bf9b3bf21b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 478722/478722 [00:04<00:00, 97595.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'T': 17125866,\n",
       "             'C': 16246533,\n",
       "             'A': 14510124,\n",
       "             'G': 8819707,\n",
       "             'N': 1406})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_bases(seqs):\n",
    "    chars = defaultdict(int)\n",
    "    for seq in tqdm(seqs):\n",
    "        for base in seq:\n",
    "            chars[base] += 1\n",
    "    return chars\n",
    "\n",
    "\n",
    "count_bases(SEQS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c50dc-f474-45ab-b6e3-2b946de3d30c",
   "metadata": {},
   "source": [
    "## Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2232c39-8622-43ea-b628-4d90afe7a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7af5dbaf-8541-4cfa-9d9a-9768def6b42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    SEQS,\n",
    "    vocab_size=consts.VOCAB_SIZE,\n",
    "    min_frequency=consts.MIN_FREQ,\n",
    "    special_tokens=consts.SPECIAL_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d426993b-5007-4d18-aa8b-4a4f91d1061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = BertProcessing(\n",
    "    (consts.EOS, tokenizer.token_to_id(consts.EOS)),\n",
    "    (consts.BOS, tokenizer.token_to_id(consts.BOS)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82b62c-2b63-4c63-a3b6-d53bfb2fe6a6",
   "metadata": {},
   "source": [
    "## Get tokenized lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5d79766-b595-4c25-a418-dbe949f34131",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = defaultdict(int)\n",
    "\n",
    "STEP = 1024\n",
    "\n",
    "SEQS = [s for s in aug_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cfdc029-02df-461f-8f17-20a534e0e783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:05<00:00, 85.12it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(SEQS), STEP)):\n",
    "    batch = tokenizer.encode_batch(SEQS[i:i + STEP])\n",
    "    for tokens in batch:\n",
    "        t_len = len(tokens)\n",
    "        lengths[t_len] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c32def98-6330-4d0d-91d4-f711f8b6459e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 4),\n",
       " (4, 106),\n",
       " (5, 997),\n",
       " (6, 3777),\n",
       " (7, 8654),\n",
       " (8, 15568),\n",
       " (9, 23642),\n",
       " (10, 31235),\n",
       " (11, 35884),\n",
       " (12, 37164),\n",
       " (13, 36703),\n",
       " (14, 34006),\n",
       " (15, 32256),\n",
       " (16, 29134),\n",
       " (17, 25957),\n",
       " (18, 23127),\n",
       " (19, 20332),\n",
       " (20, 17025),\n",
       " (21, 14585),\n",
       " (22, 12201),\n",
       " (23, 10293),\n",
       " (24, 8489),\n",
       " (25, 7324),\n",
       " (26, 6153),\n",
       " (27, 4882),\n",
       " (28, 4035),\n",
       " (29, 3546),\n",
       " (30, 3022),\n",
       " (31, 2433),\n",
       " (32, 2112),\n",
       " (33, 1951),\n",
       " (34, 2212),\n",
       " (35, 1603),\n",
       " (36, 1203),\n",
       " (37, 915),\n",
       " (38, 821),\n",
       " (39, 1075),\n",
       " (40, 848),\n",
       " (41, 725),\n",
       " (42, 724),\n",
       " (43, 757),\n",
       " (44, 795),\n",
       " (45, 650),\n",
       " (46, 547),\n",
       " (47, 473),\n",
       " (48, 422),\n",
       " (49, 505),\n",
       " (50, 587),\n",
       " (51, 581),\n",
       " (52, 436),\n",
       " (53, 374),\n",
       " (54, 537),\n",
       " (55, 593),\n",
       " (56, 687),\n",
       " (57, 620),\n",
       " (58, 365),\n",
       " (59, 366),\n",
       " (60, 422),\n",
       " (61, 448),\n",
       " (62, 412),\n",
       " (63, 382),\n",
       " (64, 227),\n",
       " (65, 252),\n",
       " (66, 271),\n",
       " (67, 111),\n",
       " (68, 59),\n",
       " (69, 48),\n",
       " (70, 53),\n",
       " (71, 13),\n",
       " (72, 2),\n",
       " (73, 1),\n",
       " (74, 2),\n",
       " (75, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lengths.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c66629-38e8-4320-952d-9164ed1ac5a8",
   "metadata": {},
   "source": [
    "Given the above I'm going to use a maximum sequence length of x tokens below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a3c5f97-5594-4be0-8b34-c4427dd0d179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consts.MAX_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b51172-4f20-4176-ad4e-f7496ac5d384",
   "metadata": {},
   "source": [
    "## Finalize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a090603-177f-46f8-a01c-96a95a7bbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_padding(\n",
    "    pad_token=consts.PAD,\n",
    "    pad_id=tokenizer.token_to_id(consts.PAD),\n",
    "    length=consts.MAX_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c602a02-e267-4624-b589-c04a5578dfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'TCAACCAATTGTG',\n",
       " 'TAC',\n",
       " 'TCGCC',\n",
       " 'GCACTGGAGGTGTAG',\n",
       " 'AGTG',\n",
       " 'ATATT',\n",
       " 'GCCC',\n",
       " 'AAAA',\n",
       " 'ATAG',\n",
       " 'AG',\n",
       " 'AACCAACC',\n",
       " 'GAACTACTCCATTAAAATG',\n",
       " 'TCGCGATTACGAGGCAG',\n",
       " 'TGAG',\n",
       " 'TCC',\n",
       " 'TTCCTCC',\n",
       " '</s>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.encode(SEQS[0])\n",
    "encoded.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8449cca-1e69-4310-99c4-0697dcc04a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(str(consts.MT_DIR / 'tokenizer.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f125494-03b4-4cca-ac69-bc4404f45402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
