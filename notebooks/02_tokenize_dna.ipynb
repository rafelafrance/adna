{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d323c0c-226b-4bfc-9597-434bac7f303c",
   "metadata": {},
   "source": [
    "# Tokenize DNA using Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189d9926-2bc9-402d-94b1-4b02dbd1b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e1db56-3340-48a6-b5ef-8a5a97030e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "from adna.pylib import consts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d2a94-cded-44f8-8515-604dfa165e47",
   "metadata": {},
   "source": [
    "## What characters are used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0f387b-48fd-4635-8209-34700c66a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(consts.SQL) as cxn:\n",
    "    RECS = pd.read_sql('select * from seqs', cxn)\n",
    "\n",
    "SEQS = RECS.seq.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c8bcb7-18f1-4b41-92ec-744769deac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHARS = set()\n",
    "\n",
    "# for seq in tqdm(SEQS):\n",
    "#     CHARS |= set(seq)\n",
    "\n",
    "# CHARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c50dc-f474-45ab-b6e3-2b946de3d30c",
   "metadata": {},
   "source": [
    "## Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2232c39-8622-43ea-b628-4d90afe7a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7af5dbaf-8541-4cfa-9d9a-9768def6b42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    SEQS,\n",
    "    vocab_size=consts.VOCAB_SIZE,\n",
    "    min_frequency=consts.MIN_FREQ,\n",
    "    special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'],\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d426993b-5007-4d18-aa8b-4a4f91d1061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82b62c-2b63-4c63-a3b6-d53bfb2fe6a6",
   "metadata": {},
   "source": [
    "## Get tokenized lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d79766-b595-4c25-a418-dbe949f34131",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = defaultdict(int)\n",
    "\n",
    "STEP = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cfdc029-02df-461f-8f17-20a534e0e783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10739/10739 [01:35<00:00, 112.78it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(SEQS), STEP)):\n",
    "    batch = tokenizer.encode_batch(SEQS[i:i+STEP])\n",
    "    for tokens in batch:\n",
    "        t_len = len(tokens)\n",
    "        lengths[t_len] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c32def98-6330-4d0d-91d4-f711f8b6459e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 741832),\n",
       " (4, 32058),\n",
       " (5, 259014),\n",
       " (6, 695258),\n",
       " (7, 889423),\n",
       " (8, 1030881),\n",
       " (9, 893056),\n",
       " (10, 906854),\n",
       " (11, 843972),\n",
       " (12, 771778),\n",
       " (13, 633199),\n",
       " (14, 564991),\n",
       " (15, 445771),\n",
       " (16, 384250),\n",
       " (17, 322374),\n",
       " (18, 254745),\n",
       " (19, 193521),\n",
       " (20, 175467),\n",
       " (21, 144669),\n",
       " (22, 114311),\n",
       " (23, 99520),\n",
       " (24, 76424),\n",
       " (25, 75970),\n",
       " (26, 59788),\n",
       " (27, 53674),\n",
       " (28, 28032),\n",
       " (29, 39348),\n",
       " (30, 22613),\n",
       " (31, 29175),\n",
       " (32, 23045),\n",
       " (33, 19974),\n",
       " (34, 10776),\n",
       " (35, 9443),\n",
       " (36, 5430),\n",
       " (37, 7278),\n",
       " (38, 9772),\n",
       " (39, 9696),\n",
       " (40, 8498),\n",
       " (41, 3524),\n",
       " (42, 5417),\n",
       " (43, 7490),\n",
       " (44, 5788),\n",
       " (45, 5238),\n",
       " (46, 4137),\n",
       " (47, 4136),\n",
       " (48, 2821),\n",
       " (49, 6887),\n",
       " (50, 4492),\n",
       " (51, 6758),\n",
       " (52, 9027),\n",
       " (53, 2973),\n",
       " (54, 2930),\n",
       " (55, 1781),\n",
       " (56, 4059),\n",
       " (57, 5665),\n",
       " (58, 3075),\n",
       " (59, 1182),\n",
       " (60, 3496),\n",
       " (61, 4378),\n",
       " (62, 5777),\n",
       " (63, 3526),\n",
       " (64, 1074),\n",
       " (65, 832),\n",
       " (66, 1035),\n",
       " (67, 1494),\n",
       " (68, 835),\n",
       " (69, 80),\n",
       " (70, 61),\n",
       " (71, 598),\n",
       " (72, 78),\n",
       " (73, 9),\n",
       " (74, 2),\n",
       " (75, 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lengths.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c66629-38e8-4320-952d-9164ed1ac5a8",
   "metadata": {},
   "source": [
    "Given the above I'm going to use a sequence length of x tokens below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a3c5f97-5594-4be0-8b34-c4427dd0d179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consts.SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b51172-4f20-4176-ad4e-f7496ac5d384",
   "metadata": {},
   "source": [
    "## Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8449cca-1e69-4310-99c4-0697dcc04a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/UF46992/vocab.json', '../data/UF46992/merges.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(str(consts.SUB_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f125494-03b4-4cca-ac69-bc4404f45402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
