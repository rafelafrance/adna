{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d323c0c-226b-4bfc-9597-434bac7f303c",
   "metadata": {},
   "source": [
    "# Tokenize DNA using Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189d9926-2bc9-402d-94b1-4b02dbd1b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e1db56-3340-48a6-b5ef-8a5a97030e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "from adna.pylib import consts, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d2a94-cded-44f8-8515-604dfa165e47",
   "metadata": {},
   "source": [
    "## What characters are used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603922f0-b6eb-4c4b-86ee-088656c4cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_dataset = datasets.ADnaDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31f420d-8738-4a5c-a376-bf9b3bf21b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 478722/478722 [00:04<00:00, 104429.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'T': 17125866,\n",
       "             'C': 16246533,\n",
       "             'A': 14510124,\n",
       "             'G': 8819707,\n",
       "             'N': 1406})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_bases(dataset):\n",
    "    chars = defaultdict(int)\n",
    "    for seq in tqdm(dataset):\n",
    "        for base in seq:\n",
    "            chars[base] += 1\n",
    "    return chars\n",
    "\n",
    "\n",
    "count_bases(counts_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c50dc-f474-45ab-b6e3-2b946de3d30c",
   "metadata": {},
   "source": [
    "## Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2232c39-8622-43ea-b628-4d90afe7a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "489085ad-8ab3-47e3-b95a-216310c9de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_seqs = datasets.ADnaDataset(\n",
    "    rev_comp_rate=consts.REV_COMP_RATE, to_n_rate=consts.TO_N_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7af5dbaf-8541-4cfa-9d9a-9768def6b42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    aug_seqs,\n",
    "    vocab_size=consts.VOCAB_SIZE,\n",
    "    min_frequency=consts.MIN_FREQ,\n",
    "    special_tokens=consts.SPECIAL_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d426993b-5007-4d18-aa8b-4a4f91d1061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = BertProcessing(\n",
    "    (consts.EOS, tokenizer.token_to_id(consts.EOS)),\n",
    "    (consts.BOS, tokenizer.token_to_id(consts.BOS)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82b62c-2b63-4c63-a3b6-d53bfb2fe6a6",
   "metadata": {},
   "source": [
    "## Get tokenized lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5d79766-b595-4c25-a418-dbe949f34131",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = defaultdict(int)\n",
    "\n",
    "STEP = 1024\n",
    "\n",
    "SEQS = [s for s in aug_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cfdc029-02df-461f-8f17-20a534e0e783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:06<00:00, 75.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(SEQS), STEP)):\n",
    "    batch = tokenizer.encode_batch(SEQS[i:i + STEP])\n",
    "    for tokens in batch:\n",
    "        t_len = len(tokens)\n",
    "        lengths[t_len] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c32def98-6330-4d0d-91d4-f711f8b6459e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1),\n",
       " (4, 21),\n",
       " (5, 214),\n",
       " (6, 866),\n",
       " (7, 1954),\n",
       " (8, 3795),\n",
       " (9, 6186),\n",
       " (10, 9084),\n",
       " (11, 12281),\n",
       " (12, 15616),\n",
       " (13, 19500),\n",
       " (14, 22197),\n",
       " (15, 24753),\n",
       " (16, 26516),\n",
       " (17, 28207),\n",
       " (18, 28457),\n",
       " (19, 27733),\n",
       " (20, 27018),\n",
       " (21, 25533),\n",
       " (22, 23775),\n",
       " (23, 21760),\n",
       " (24, 20248),\n",
       " (25, 17895),\n",
       " (26, 15874),\n",
       " (27, 13483),\n",
       " (28, 11610),\n",
       " (29, 9802),\n",
       " (30, 8447),\n",
       " (31, 7088),\n",
       " (32, 6037),\n",
       " (33, 4980),\n",
       " (34, 4091),\n",
       " (35, 3497),\n",
       " (36, 2885),\n",
       " (37, 2434),\n",
       " (38, 2215),\n",
       " (39, 1903),\n",
       " (40, 1627),\n",
       " (41, 1393),\n",
       " (42, 1275),\n",
       " (43, 1113),\n",
       " (44, 1040),\n",
       " (45, 989),\n",
       " (46, 887),\n",
       " (47, 888),\n",
       " (48, 819),\n",
       " (49, 808),\n",
       " (50, 801),\n",
       " (51, 773),\n",
       " (52, 728),\n",
       " (53, 659),\n",
       " (54, 591),\n",
       " (55, 573),\n",
       " (56, 638),\n",
       " (57, 520),\n",
       " (58, 512),\n",
       " (59, 524),\n",
       " (60, 527),\n",
       " (61, 457),\n",
       " (62, 484),\n",
       " (63, 447),\n",
       " (64, 404),\n",
       " (65, 344),\n",
       " (66, 252),\n",
       " (67, 250),\n",
       " (68, 177),\n",
       " (69, 119),\n",
       " (70, 70),\n",
       " (71, 35),\n",
       " (72, 22),\n",
       " (73, 12),\n",
       " (74, 5),\n",
       " (75, 3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lengths.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c66629-38e8-4320-952d-9164ed1ac5a8",
   "metadata": {},
   "source": [
    "Given the above I'm going to use a maximum sequence length of x tokens below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a3c5f97-5594-4be0-8b34-c4427dd0d179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consts.MAX_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b51172-4f20-4176-ad4e-f7496ac5d384",
   "metadata": {},
   "source": [
    "## Finalize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a090603-177f-46f8-a01c-96a95a7bbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_padding(\n",
    "    pad_token=consts.PAD,\n",
    "    pad_id=tokenizer.token_to_id(consts.PAD),\n",
    "    length=consts.MAX_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c602a02-e267-4624-b589-c04a5578dfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'GGAGG',\n",
       " 'AAGG',\n",
       " 'ACTC',\n",
       " 'ACTGCC',\n",
       " 'NCG',\n",
       " 'TAATCGCGACATTTTAATGGAGTAGTTCGGTTGG',\n",
       " 'TTCTC',\n",
       " 'TATT',\n",
       " 'TTTGGGCAA',\n",
       " 'TATC',\n",
       " 'ACTCTACACCTCC',\n",
       " 'AGTGCGGC',\n",
       " 'GAG',\n",
       " 'TACACAATTGGTTG',\n",
       " 'A',\n",
       " '</s>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.encode(SEQS[0])\n",
    "encoded.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8449cca-1e69-4310-99c4-0697dcc04a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(str(consts.SUB_DIR / 'tokenizer.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f125494-03b4-4cca-ac69-bc4404f45402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
