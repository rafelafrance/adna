{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d323c0c-226b-4bfc-9597-434bac7f303c",
   "metadata": {},
   "source": [
    "# Tokenize DNA using Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189d9926-2bc9-402d-94b1-4b02dbd1b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e1db56-3340-48a6-b5ef-8a5a97030e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "from adna.pylib import consts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d2a94-cded-44f8-8515-604dfa165e47",
   "metadata": {},
   "source": [
    "## What characters are used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0f387b-48fd-4635-8209-34700c66a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(consts.SQL) as cxn:\n",
    "    RECS = pd.read_sql('select * from seqs', cxn)\n",
    "\n",
    "SEQS = RECS.seq.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c8bcb7-18f1-4b41-92ec-744769deac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10996536/10996536 [00:21<00:00, 514721.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A', 'C', 'G', 'N', 'T'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHARS = set()\n",
    "\n",
    "for seq in tqdm(SEQS):\n",
    "    CHARS |= set(seq)\n",
    "\n",
    "CHARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c50dc-f474-45ab-b6e3-2b946de3d30c",
   "metadata": {},
   "source": [
    "## Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2232c39-8622-43ea-b628-4d90afe7a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7af5dbaf-8541-4cfa-9d9a-9768def6b42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    SEQS,\n",
    "    vocab_size=consts.VOCAB_SIZE,\n",
    "    min_frequency=consts.MIN_FREQ,\n",
    "    special_tokens=consts.SPECIAL_TOKENS,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d426993b-5007-4d18-aa8b-4a4f91d1061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = BertProcessing(\n",
    "    (consts.EOS, tokenizer.token_to_id(consts.EOS)),\n",
    "    (consts.BOS, tokenizer.token_to_id(consts.BOS)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82b62c-2b63-4c63-a3b6-d53bfb2fe6a6",
   "metadata": {},
   "source": [
    "## Get tokenized lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d79766-b595-4c25-a418-dbe949f34131",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = defaultdict(int)\n",
    "\n",
    "STEP = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cfdc029-02df-461f-8f17-20a534e0e783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 10739/10739 [01:34<00:00, 113.78it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(SEQS), STEP)):\n",
    "    batch = tokenizer.encode_batch(SEQS[i:i+STEP])\n",
    "    for tokens in batch:\n",
    "        t_len = len(tokens)\n",
    "        lengths[t_len] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c32def98-6330-4d0d-91d4-f711f8b6459e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 456132),\n",
       " (4, 50481),\n",
       " (5, 160323),\n",
       " (6, 496402),\n",
       " (7, 728419),\n",
       " (8, 974892),\n",
       " (9, 1006936),\n",
       " (10, 885681),\n",
       " (11, 735098),\n",
       " (12, 937537),\n",
       " (13, 658147),\n",
       " (14, 566957),\n",
       " (15, 478597),\n",
       " (16, 408926),\n",
       " (17, 342617),\n",
       " (18, 307897),\n",
       " (19, 243092),\n",
       " (20, 224337),\n",
       " (21, 188488),\n",
       " (22, 165525),\n",
       " (23, 119108),\n",
       " (24, 109520),\n",
       " (25, 92547),\n",
       " (26, 84722),\n",
       " (27, 87372),\n",
       " (28, 59555),\n",
       " (29, 61469),\n",
       " (30, 43390),\n",
       " (31, 46996),\n",
       " (32, 32887),\n",
       " (33, 35773),\n",
       " (34, 18087),\n",
       " (35, 14451),\n",
       " (36, 14450),\n",
       " (37, 4245),\n",
       " (38, 12010),\n",
       " (39, 4725),\n",
       " (40, 10594),\n",
       " (41, 3415),\n",
       " (42, 6593),\n",
       " (43, 6486),\n",
       " (44, 5224),\n",
       " (45, 5528),\n",
       " (46, 7570),\n",
       " (47, 6714),\n",
       " (48, 4085),\n",
       " (49, 3871),\n",
       " (50, 4719),\n",
       " (51, 9124),\n",
       " (52, 4623),\n",
       " (53, 4656),\n",
       " (54, 3473),\n",
       " (55, 4700),\n",
       " (56, 2148),\n",
       " (57, 6445),\n",
       " (58, 3351),\n",
       " (59, 4302),\n",
       " (60, 5204),\n",
       " (61, 2133),\n",
       " (62, 7285),\n",
       " (63, 1215),\n",
       " (64, 6416),\n",
       " (65, 1280),\n",
       " (66, 3008),\n",
       " (67, 806),\n",
       " (68, 990),\n",
       " (69, 1321),\n",
       " (70, 669),\n",
       " (71, 669),\n",
       " (72, 37),\n",
       " (73, 19),\n",
       " (74, 64),\n",
       " (75, 4),\n",
       " (76, 3),\n",
       " (78, 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lengths.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c66629-38e8-4320-952d-9164ed1ac5a8",
   "metadata": {},
   "source": [
    "Given the above I'm going to use a maximum sequence length of x tokens below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a3c5f97-5594-4be0-8b34-c4427dd0d179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consts.MAX_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b51172-4f20-4176-ad4e-f7496ac5d384",
   "metadata": {},
   "source": [
    "## Finalize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a090603-177f-46f8-a01c-96a95a7bbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_padding(\n",
    "    pad_token=consts.PAD,\n",
    "    pad_id=tokenizer.token_to_id(consts.PAD),\n",
    "    length=consts.MAX_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c602a02-e267-4624-b589-c04a5578dfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'GCCTT',\n",
       " 'CAAGGATGAATTAATGATACGGTTTCGGGTGTAAGCCGGGCGTTCATTTTAACACTGATGCACTTGTAATTACATTTGGTT',\n",
       " 'ATGGTATATCC',\n",
       " 'ACCC',\n",
       " '</s>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.encode(SEQS[0])\n",
    "encoded.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8449cca-1e69-4310-99c4-0697dcc04a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(str(consts.SUB_DIR / 'tokenizer.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f125494-03b4-4cca-ac69-bc4404f45402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
