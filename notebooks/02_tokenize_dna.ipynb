{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d323c0c-226b-4bfc-9597-434bac7f303c",
   "metadata": {},
   "source": [
    "# Tokenize DNA using Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189d9926-2bc9-402d-94b1-4b02dbd1b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e1db56-3340-48a6-b5ef-8a5a97030e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "from adna.pylib import consts\n",
    "from adna.pylib import dataset_utils as du"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84940c-5962-420a-892e-6f9e780bb6b3",
   "metadata": {},
   "source": [
    "## Tokenizer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323755b-6d02-458d-9f5d-f8bd94d3cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f75b2-fa91-4e79-802d-bd9685b25c2a",
   "metadata": {},
   "source": [
    "## Build sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "489085ad-8ab3-47e3-b95a-216310c9de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQS, _ = du.read_seqs_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ca408-0f8f-41f4-a32e-26b86134ca4c",
   "metadata": {},
   "source": [
    "Data augmentation use reverse complements so make sure they're represented in the token set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4045cb0b-4e44-4d4c-979d-5c828e3a7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQS += [du.rev_comp(seq) for seq in SEQS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d2a94-cded-44f8-8515-604dfa165e47",
   "metadata": {},
   "source": [
    "## What characters are used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31f420d-8738-4a5c-a376-bf9b3bf21b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 957444/957444 [00:09<00:00, 95829.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'T': 31635990,\n",
       "             'C': 25066240,\n",
       "             'A': 31635990,\n",
       "             'G': 25066240,\n",
       "             'N': 2812})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_bases(seqs):\n",
    "    chars = defaultdict(int)\n",
    "    for seq in tqdm(seqs):\n",
    "        for base in seq:\n",
    "            chars[base] += 1\n",
    "    return chars\n",
    "\n",
    "\n",
    "count_bases(SEQS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c50dc-f474-45ab-b6e3-2b946de3d30c",
   "metadata": {},
   "source": [
    "## Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2232c39-8622-43ea-b628-4d90afe7a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7af5dbaf-8541-4cfa-9d9a-9768def6b42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    SEQS,\n",
    "    vocab_size=consts.VOCAB_SIZE,\n",
    "    min_frequency=MIN_FREQ,\n",
    "    special_tokens=consts.SPECIAL_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d426993b-5007-4d18-aa8b-4a4f91d1061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = BertProcessing(\n",
    "    (consts.EOS, tokenizer.token_to_id(consts.EOS)),\n",
    "    (consts.BOS, tokenizer.token_to_id(consts.BOS)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82b62c-2b63-4c63-a3b6-d53bfb2fe6a6",
   "metadata": {},
   "source": [
    "## Get tokenized lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d79766-b595-4c25-a418-dbe949f34131",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = defaultdict(int)\n",
    "\n",
    "STEP = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cfdc029-02df-461f-8f17-20a534e0e783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 936/936 [00:11<00:00, 84.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(SEQS), STEP)):\n",
    "    batch = tokenizer.encode_batch(SEQS[i : i + STEP])\n",
    "    for tokens in batch:\n",
    "        t_len = len(tokens)\n",
    "        lengths[t_len] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c32def98-6330-4d0d-91d4-f711f8b6459e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 6),\n",
       " (4, 125),\n",
       " (5, 1166),\n",
       " (6, 4887),\n",
       " (7, 11385),\n",
       " (8, 19592),\n",
       " (9, 30016),\n",
       " (10, 42025),\n",
       " (11, 54545),\n",
       " (12, 61554),\n",
       " (13, 65535),\n",
       " (14, 64280),\n",
       " (15, 61588),\n",
       " (16, 58187),\n",
       " (17, 53622),\n",
       " (18, 49117),\n",
       " (19, 45036),\n",
       " (20, 41231),\n",
       " (21, 37195),\n",
       " (22, 33417),\n",
       " (23, 29641),\n",
       " (24, 26398),\n",
       " (25, 21967),\n",
       " (26, 19725),\n",
       " (27, 16557),\n",
       " (28, 13639),\n",
       " (29, 11150),\n",
       " (30, 9131),\n",
       " (31, 7402),\n",
       " (32, 6328),\n",
       " (33, 5145),\n",
       " (34, 4538),\n",
       " (35, 4181),\n",
       " (36, 3782),\n",
       " (37, 2962),\n",
       " (38, 2490),\n",
       " (39, 2252),\n",
       " (40, 2188),\n",
       " (41, 2243),\n",
       " (42, 2002),\n",
       " (43, 1931),\n",
       " (44, 2092),\n",
       " (45, 1886),\n",
       " (46, 1612),\n",
       " (47, 1183),\n",
       " (48, 1213),\n",
       " (49, 1372),\n",
       " (50, 1598),\n",
       " (51, 1336),\n",
       " (52, 1086),\n",
       " (53, 1226),\n",
       " (54, 1406),\n",
       " (55, 1571),\n",
       " (56, 936),\n",
       " (57, 771),\n",
       " (58, 1008),\n",
       " (59, 1360),\n",
       " (60, 978),\n",
       " (61, 780),\n",
       " (62, 1009),\n",
       " (63, 897),\n",
       " (64, 609),\n",
       " (65, 531),\n",
       " (66, 411),\n",
       " (67, 215),\n",
       " (68, 117),\n",
       " (69, 66),\n",
       " (70, 8),\n",
       " (71, 5),\n",
       " (72, 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lengths.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c66629-38e8-4320-952d-9164ed1ac5a8",
   "metadata": {},
   "source": [
    "Given the above I'm going to use a maximum sequence length of x tokens below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a3c5f97-5594-4be0-8b34-c4427dd0d179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consts.MAX_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b51172-4f20-4176-ad4e-f7496ac5d384",
   "metadata": {},
   "source": [
    "## Finalize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a090603-177f-46f8-a01c-96a95a7bbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_padding(\n",
    "    pad_token=consts.PAD,\n",
    "    pad_id=tokenizer.token_to_id(consts.PAD),\n",
    "    length=consts.MAX_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c602a02-e267-4624-b589-c04a5578dfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'TC',\n",
       " 'AACCAA',\n",
       " 'TTGTG',\n",
       " 'TAC',\n",
       " 'TCGCC',\n",
       " 'GCAC',\n",
       " 'TGGAGGTGTAG',\n",
       " 'AGTG',\n",
       " 'ATATTG',\n",
       " 'CCCAAAA',\n",
       " 'ATAG',\n",
       " 'AGAACC',\n",
       " 'AACCGAACTACTCCATTAAAATGTCGCGATTACGAGGC',\n",
       " 'AGTGAG',\n",
       " 'TCC',\n",
       " 'TTCCTCC',\n",
       " '</s>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.encode(SEQS[0])\n",
    "encoded.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8449cca-1e69-4310-99c4-0697dcc04a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(str(consts.MT_DIR / \"tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f125494-03b4-4cca-ac69-bc4404f45402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
